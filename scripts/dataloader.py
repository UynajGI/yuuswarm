import json
import os
import re
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# å°è¯•å¯¼å…¥è¿›åº¦æ¡ï¼Œæ²¡æœ‰å°±ç”¨ç®€æ˜“ç‰ˆ
try:
    from tqdm import tqdm
except ImportError:

    def tqdm(iterator, **kwargs):
        return iterator


# ================= é…ç½® =================
INPUT_ROOT = Path(__file__).resolve().parent.parent / "output" / "scan_JK"
OUTPUT_ROOT = Path(__file__).resolve().parent.parent / "processed_data_lake"


def get_slurm_cores():
    """å°è¯•è·å– Slurm åˆ†é…çš„æ ¸æ•°ï¼Œå¦‚æœæ²¡åœ¨ Slurm é‡Œåˆ™è¿”å›ç‰©ç†æ ¸æ•°"""
    # 1. ä¼˜å…ˆå°è¯•è¯»å– Linux è¿›ç¨‹äº²å’Œæ€§ (æœ€å‡†ç¡®ï¼Œèƒ½çœ‹åˆ°å®é™…èƒ½ç”¨çš„æ ¸)
    try:
        return len(os.sched_getaffinity(0))
    except AttributeError:
        pass

    # 2. å°è¯•è¯»å– Slurm ç¯å¢ƒå˜é‡
    slurm_cpus = os.environ.get("SLURM_CPUS_PER_TASK")
    if slurm_cpus:
        return int(slurm_cpus)

    # 3.ä¸ä»…å¦‚æ­¤ï¼Œé˜²æ­¢ OOMï¼Œè¿˜æ˜¯ä¿å®ˆä¸€ç‚¹
    return max(1, os.cpu_count() - 1)


# è®¾ç½® Worker æ•°é‡
NUM_WORKERS = get_slurm_cores()


def flatten_json(y):
    """é€šç”¨é€’å½’å‡½æ•°ï¼šæŠŠä»»æ„åµŒå¥—çš„ JSON å±•å¹³ã€‚"""
    out = {}

    def flatten(x, name=""):
        if type(x) is dict:
            for a in x:
                flatten(x[a], name + a + ".")
        else:
            out[name[:-1]] = x

    flatten(y)
    return out


def generate_dynamic_cols(d, d_s):
    """ç”Ÿæˆ x_0, x_1... è¿™ç§åˆ—å"""
    cols = []
    for p in ["x", "v"]:
        for k in range(d):
            cols.append(f"{p}_{k}")
    for p in ["s", "w"]:
        for k in range(d_s):
            cols.append(f"{p}_{k}")
    return cols


def process_experiment_data(exp_dir):
    """
    æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šè¯»å– -> åˆå¹¶ -> ç”Ÿæˆ DataFrame
    """
    exp_id = exp_dir.name

    # 1. è¯»å– Config
    cfg_path = exp_dir / "config_snapshot.json"
    if not cfg_path.exists():
        return None

    with open(cfg_path, "r") as f:
        config = json.load(f)

    flat_config = flatten_json(config)
    d = config.get("d", 2)
    d_s = config.get("d_s", 2)
    expected_dims = 2 * d + 2 * d_s

    # 2. è¯»å– NPZ
    npz_files = sorted(
        list(exp_dir.glob("simulation_part_*.npz")),
        key=lambda f: int(re.search(r"part_(\d+)", f.name).group(1)),
    )
    if not npz_files:
        return None

    # åˆå¹¶æ•°ç»„
    all_t, all_traj = [], []
    for f in npz_files:
        try:
            dat = np.load(f)
            all_t.append(dat["time"])
            all_traj.append(dat["trajectory"])
        except Exception:
            continue

    if not all_traj:
        return None

    t_arr = np.concatenate(all_t)
    traj_arr = np.concatenate(all_traj)  # (Steps, N, Dims)

    steps, n, total_dims = traj_arr.shape
    if total_dims != expected_dims:
        return None  # ç»´åº¦ä¸åŒ¹é…ç›´æ¥æ”¾å¼ƒ

    # 3. æ„å»º DataFrame
    flat_traj = traj_arr.reshape(-1, total_dims)

    data_dict = {
        "t": np.repeat(t_arr, n).astype(np.float32),
        "p_id": np.tile(np.arange(n), steps).astype(np.int32),
        "exp_id": exp_id,
    }

    col_names = generate_dynamic_cols(d, d_s)
    for idx, name in enumerate(col_names):
        data_dict[name] = flat_traj[:, idx].astype(np.float32)

    df = pd.DataFrame(data_dict)

    # 4. å¹¿æ’­å‚æ•°
    for key, val in flat_config.items():
        if isinstance(val, (int, float, bool, str)) and len(str(val)) < 100:
            clean_key = key.replace("params.", "")
            if clean_key == "n":
                continue  # é¿å…è¦†ç›–å®é™…çš„ n

            df[clean_key] = val
            if isinstance(val, float):
                df[clean_key] = df[clean_key].astype(np.float32)
            elif isinstance(val, int):
                df[clean_key] = df[clean_key].astype(np.int32)

    df["n"] = n
    return df, flat_config


def worker_task(exp_dir):
    """
    å•ä¸ª Worker è¿›ç¨‹æ‰§è¡Œçš„ä»»åŠ¡ï¼šå¤„ç† -> ä¿å­˜ -> è¿”å›å…ƒæ•°æ®
    æ³¨æ„ï¼šä¸åœ¨è¿›ç¨‹é—´ä¼ é€’ huge DataFrameï¼Œç›´æ¥åœ¨å­è¿›ç¨‹å­˜ç›˜ï¼
    """
    try:
        res = process_experiment_data(exp_dir)
        if res is None:
            return None

        df, config_dict = res
        exp_id = exp_dir.name

        # ä¿å­˜ Parquet (è¿™æ˜¯è€—æ—¶æ“ä½œï¼Œè¦åœ¨ Worker é‡Œåš)
        save_path = OUTPUT_ROOT / f"{exp_id}.parquet"
        df.to_parquet(save_path, index=False, compression="zstd")

        # å‡†å¤‡è¿”å›ç»™ä¸»è¿›ç¨‹çš„è½»é‡çº§å…ƒæ•°æ®
        config_dict["exp_id"] = exp_id
        config_dict["file_path"] = str(save_path.name)

        return config_dict

    except Exception as e:
        # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢ç‚¸å Pool
        print(f"\nâŒ Error in {exp_dir.name}: {e}")
        return None


# ... (å‰é¢çš„ import å’Œå‡½æ•°å®šä¹‰ä¿æŒä¸å˜) ...

# å…¨å±€é»˜è®¤é…ç½®
FORCE_UPDATE = False


def main():
    # --- 1. å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ ---
    global FORCE_UPDATE
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--force":
        FORCE_UPDATE = True
        print("âš ï¸  FORCE MODE: All existing files will be overwritten!")

    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
    if not INPUT_ROOT.exists():
        print("Input dir not found")
        return

    # è·å–æ‰€æœ‰å®éªŒæ–‡ä»¶å¤¹
    exp_dirs = [d for d in INPUT_ROOT.iterdir() if d.is_dir()]

    # --- 2. ç­›é€‰éœ€è¦å¤„ç†çš„ä»»åŠ¡ (å¢é‡é€»è¾‘) ---
    tasks_to_run = []
    skipped_count = 0

    print(f"ğŸ” Scanning {len(exp_dirs)} directories...")

    for d in exp_dirs:
        exp_id = d.name
        target_parquet = OUTPUT_ROOT / f"{exp_id}.parquet"

        # æ ¸å¿ƒåˆ¤æ–­é€»è¾‘
        if target_parquet.exists() and not FORCE_UPDATE:
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œä¸”æ²¡æœ‰å¼€å¯å¼ºåˆ¶æ›´æ–°ï¼Œå°±è·³è¿‡
            skipped_count += 1
            continue
        else:
            # å¦åˆ™åŠ å…¥ä»»åŠ¡åˆ—è¡¨
            tasks_to_run.append(d)

    print(
        f"ğŸ“‹ Plan: Process {len(tasks_to_run)} new/changed, Skip {skipped_count} existing."
    )

    if not tasks_to_run:
        print("âœ… Nothing to do.")
        return

    # --- 3. å¹¶è¡Œæ‰§è¡Œ (åªè·‘ç­›é€‰å‡ºæ¥çš„ä»»åŠ¡) ---
    metadata_list = []

    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæäº¤ tasks_to_run
    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        futures = [executor.submit(worker_task, d) for d in tasks_to_run]

        for future in tqdm(as_completed(futures), total=len(tasks_to_run), unit="exp"):
            result = future.result()
            if result:
                metadata_list.append(result)

    print(f"\nğŸ’¾ Saved {len(metadata_list)} new parquet files.")

    # --- 4. ç´¢å¼•æ–‡ä»¶çš„å¤„ç† (æ³¨æ„ï¼) ---
    # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œmetadata_list åªåŒ…å«â€œæœ¬æ¬¡æ–°è·‘â€çš„å®éªŒã€‚
    # ä¸ºäº†ä¿è¯ _metadata_index.parquet åŒ…å«æ‰€æœ‰ï¼ˆæ—§+æ–°ï¼‰æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ—§çš„ç´¢å¼•è¯»è¿›æ¥åˆå¹¶

    index_path = OUTPUT_ROOT / "_metadata_index.parquet"
    final_meta_df = pd.DataFrame(metadata_list)

    if index_path.exists() and not FORCE_UPDATE:
        try:
            print("ğŸ”— Merging with existing index...")
            old_index = pd.read_parquet(index_path)
            # åˆå¹¶æ–°æ—§ç´¢å¼•ï¼Œå¹¶æ ¹æ® exp_id å»é‡ (ä¿ç•™æ–°çš„)
            if not final_meta_df.empty:
                # æ¸…ç†åˆ—åä»¥åŒ¹é…
                final_meta_df.columns = [
                    c.replace("params.", "") for c in final_meta_df.columns
                ]

                combined = pd.concat([old_index, final_meta_df])
                # drop_duplicates: æ¯”å¦‚ä½ å¼ºåˆ¶æ›´æ–°äº†æŸä¸ªæ–‡ä»¶ï¼Œè¿™é‡Œè¦æŠŠæ—§ç´¢å¼•é‡Œçš„å®ƒåˆ æ‰
                final_meta_df = combined.drop_duplicates(subset=["exp_id"], keep="last")
            else:
                final_meta_df = old_index
        except Exception as e:
            print(f"âš ï¸ Failed to read old index, creating new one: {e}")

    # æ¸…ç†åˆ—å (å¦‚æœæ˜¯å…¨æ–°ç”Ÿæˆçš„)
    if not final_meta_df.empty:
        # ç¡®ä¿åˆ—åæ²¡æœ‰ 'params.' å‰ç¼€ (å¦‚æœæ˜¯ä» metadata_list æ–°å»ºçš„)
        final_meta_df.columns = [
            c.replace("params.", "") for c in final_meta_df.columns
        ]

        final_meta_df.to_parquet(index_path, index=False)
        print(f"âœ… Index updated: {index_path} ({len(final_meta_df)} experiments)")


if __name__ == "__main__":
    import multiprocessing

    multiprocessing.freeze_support()
    main()
